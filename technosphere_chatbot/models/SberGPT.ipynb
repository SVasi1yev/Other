{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Args():\n",
    "#     def __init__(self):\n",
    "#         self.output_dir = 'output-small'\n",
    "#         self.model_type = 'gpt2'\n",
    "#         self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "#         self.config_name = 'microsoft/DialoGPT-small'\n",
    "#         self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "#         self.cache_dir = 'cached'\n",
    "#         self.block_size = 512\n",
    "#         self.do_train = True\n",
    "#         self.do_eval = True\n",
    "#         self.evaluate_during_training = False\n",
    "#         self.per_gpu_train_batch_size = 4\n",
    "#         self.per_gpu_eval_batch_size = 4\n",
    "#         self.gradient_accumulation_steps = 1\n",
    "#         self.learning_rate = 5e-5\n",
    "#         self.weight_decay = 0.0\n",
    "#         self.adam_epsilon = 1e-8\n",
    "#         self.max_grad_norm = 1.0\n",
    "#         self.num_train_epochs = 3\n",
    "#         self.max_steps = -1\n",
    "#         self.warmup_steps = 0\n",
    "#         self.logging_steps = 1000\n",
    "#         self.save_steps = 3500\n",
    "#         self.save_total_limit = None\n",
    "#         self.eval_all_checkpoints = False\n",
    "#         self.no_cuda = False\n",
    "#         self.overwrite_output_dir = True\n",
    "#         self.overwrite_cache = True\n",
    "#         self.should_continue = False\n",
    "#         self.seed = 42\n",
    "#         self.local_rank = -1\n",
    "#         self.fp16 = False\n",
    "#         self.fp16_opt_level = 'O1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import logging\n",
    "# import os\n",
    "# import pickle\n",
    "# import random\n",
    "# import re\n",
    "# import shutil\n",
    "# from typing import Dict, List, Tuple\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# from transformers import (\n",
    "#     MODEL_WITH_LM_HEAD_MAPPING,\n",
    "#     WEIGHTS_NAME,\n",
    "#     AdamW,\n",
    "#     AutoConfig,\n",
    "#     AutoModelWithLMHead,\n",
    "#     AutoTokenizer,\n",
    "#     PreTrainedModel,\n",
    "#     PreTrainedTokenizer,\n",
    "#     get_linear_schedule_with_warmup,\n",
    "# )\n",
    "\n",
    "\n",
    "# try:\n",
    "#     from torch.utils.tensorboard import SummaryWriter\n",
    "# except ImportError:\n",
    "#     from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "# model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's chat for 5 lines\n",
    "# for step in range(5):\n",
    "#     # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "#     # append the new user input tokens to the chat history\n",
    "#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "#     # generated a response while limiting the total chat history to 1000 tokens    \n",
    "#     chat_history_ids = model.generate(\n",
    "#     bot_input_ids, max_length=1000,\n",
    "#     pad_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "#     # pretty print last ouput tokens from bot\n",
    "#     print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelWithLMHead, \n",
    "    AutoTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\transformers\\modeling_auto.py:781: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt2large\")\n",
    "tokenizer.add_special_tokens({'eos_token': '<eos>'})\n",
    "model = AutoModelWithLMHead.from_pretrained(\"sberbank-ai/rugpt2large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На словах ты Лев Толстой, а на деле - говно! - с чувством сказал он и, развернувшись, вышел.\n",
      "\n",
      "- Ну и чего теперь? - спросил меня один из его\n"
     ]
    }
   ],
   "source": [
    "text = 'На словах ты Лев Толстой, а на деле'\n",
    "input_ = tokenizer.encode(text, return_tensors='pt')\n",
    "out = model.generate(\n",
    "    input_, max_length=40,\n",
    "#     pad_token_id=tokenizer.eos_token_id,  \n",
    "#     no_repeat_ngram_size=3,       \n",
    "    do_sample=True, \n",
    "    top_k=10, \n",
    "    top_p=0.9,\n",
    "    temperature = 1.0,\n",
    "    repetition_penalty = 1.0,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "# repetition_penalty = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh.</td>\n",
       "      <td>mr. bergstrom left today.</td>\n",
       "      <td>nothing.</td>\n",
       "      <td>what are you so mopey about?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and?</td>\n",
       "      <td>he's gone. forever.</td>\n",
       "      <td>oh.</td>\n",
       "      <td>mr. bergstrom left today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey, just because i don't care doesn't mean i ...</td>\n",
       "      <td>i didn't think you'd understand.</td>\n",
       "      <td>and?</td>\n",
       "      <td>he's gone. forever.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me?</td>\n",
       "      <td>i'm glad i'm not crying because i would hate ...</td>\n",
       "      <td>hey, just because i don't care doesn't mean i ...</td>\n",
       "      <td>i didn't think you'd understand.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i don't think you realize what you're saying.</td>\n",
       "      <td>yes, you! baboon, baboon, baboon, baboon!</td>\n",
       "      <td>me?</td>\n",
       "      <td>i'm glad i'm not crying because i would hate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12312</th>\n",
       "      <td>bart! there's a really mean squirrel in the ba...</td>\n",
       "      <td>face it, we're just kids. we can't afford stuf...</td>\n",
       "      <td>there is no mrs. steak. who could stay married...</td>\n",
       "      <td>why don't you talk to mrs. steak?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12313</th>\n",
       "      <td>bart's been raptured! and his crap's been cra...</td>\n",
       "      <td>bart! there's a really mean squirrel in the ba...</td>\n",
       "      <td>face it, we're just kids. we can't afford stuf...</td>\n",
       "      <td>there is no mrs. steak. who could stay married...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12314</th>\n",
       "      <td>nice doin' business with you, boy.</td>\n",
       "      <td>i can't, i sold my \"dinner futures.\"</td>\n",
       "      <td>why do you want a box of bart's baby teeth?</td>\n",
       "      <td>bart's been raptured! and his crap's been cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12315</th>\n",
       "      <td>what happened to my mini-pool table? i was tra...</td>\n",
       "      <td>and dad donated something, too.</td>\n",
       "      <td>homer!</td>\n",
       "      <td>nice doin' business with you, boy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12316</th>\n",
       "      <td>i never even got to realize this jacket was t...</td>\n",
       "      <td>we sold it to pay for a family cruise!</td>\n",
       "      <td>what happened to my mini-pool table? i was tra...</td>\n",
       "      <td>and dad donated something, too.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11578 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                response  \\\n",
       "0                                                    oh.   \n",
       "1                                                   and?   \n",
       "2      hey, just because i don't care doesn't mean i ...   \n",
       "3                                                    me?   \n",
       "4          i don't think you realize what you're saying.   \n",
       "...                                                  ...   \n",
       "12312  bart! there's a really mean squirrel in the ba...   \n",
       "12313   bart's been raptured! and his crap's been cra...   \n",
       "12314                nice doin' business with you, boy.    \n",
       "12315  what happened to my mini-pool table? i was tra...   \n",
       "12316   i never even got to realize this jacket was t...   \n",
       "\n",
       "                                                 context  \\\n",
       "0                              mr. bergstrom left today.   \n",
       "1                                    he's gone. forever.   \n",
       "2                       i didn't think you'd understand.   \n",
       "3       i'm glad i'm not crying because i would hate ...   \n",
       "4              yes, you! baboon, baboon, baboon, baboon!   \n",
       "...                                                  ...   \n",
       "12312  face it, we're just kids. we can't afford stuf...   \n",
       "12313  bart! there's a really mean squirrel in the ba...   \n",
       "12314               i can't, i sold my \"dinner futures.\"   \n",
       "12315                    and dad donated something, too.   \n",
       "12316             we sold it to pay for a family cruise!   \n",
       "\n",
       "                                               context/0  \\\n",
       "0                                               nothing.   \n",
       "1                                                    oh.   \n",
       "2                                                   and?   \n",
       "3      hey, just because i don't care doesn't mean i ...   \n",
       "4                                                    me?   \n",
       "...                                                  ...   \n",
       "12312  there is no mrs. steak. who could stay married...   \n",
       "12313  face it, we're just kids. we can't afford stuf...   \n",
       "12314        why do you want a box of bart's baby teeth?   \n",
       "12315                                             homer!   \n",
       "12316  what happened to my mini-pool table? i was tra...   \n",
       "\n",
       "                                               context/1  \n",
       "0                           what are you so mopey about?  \n",
       "1                              mr. bergstrom left today.  \n",
       "2                                    he's gone. forever.  \n",
       "3                       i didn't think you'd understand.  \n",
       "4       i'm glad i'm not crying because i would hate ...  \n",
       "...                                                  ...  \n",
       "12312                  why don't you talk to mrs. steak?  \n",
       "12313  there is no mrs. steak. who could stay married...  \n",
       "12314   bart's been raptured! and his crap's been cra...  \n",
       "12315                nice doin' business with you, boy.   \n",
       "12316                    and dad donated something, too.  \n",
       "\n",
       "[11578 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/gpt2_simpsons_homer_3_dataset.csv', sep=',')\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>scotland.</td>\n",
       "      <td>eek! -- i mean, ach! -- i mean, what are you ...</td>\n",
       "      <td>well, it used to be.</td>\n",
       "      <td>and now we wait.  so... is this your school?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12154</th>\n",
       "      <td>fine.</td>\n",
       "      <td>hey, henrietta hippo, i've got to drain the in...</td>\n",
       "      <td>girling down! girling down!</td>\n",
       "      <td>dad, where are we going? do we have a plan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>mainly your mother.</td>\n",
       "      <td>nooooooo!</td>\n",
       "      <td>your mother and i have been thinking about giv...</td>\n",
       "      <td>well, i have to coordinate, don't i?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6511</th>\n",
       "      <td>that's right. and i implore you to move your ...</td>\n",
       "      <td>the crazy mayor of new springfield?</td>\n",
       "      <td>that's right.</td>\n",
       "      <td>the mayor of new springfield?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>now all we need is our astronaut. bart, where'...</td>\n",
       "      <td>who wants some astro-lemonade?</td>\n",
       "      <td>dweeb, wonk, spaz... it's all good.</td>\n",
       "      <td>well, then, could you stop calling us nerds?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                response  \\\n",
       "4722                                           scotland.   \n",
       "12154                                              fine.   \n",
       "2695                                 mainly your mother.   \n",
       "6511    that's right. and i implore you to move your ...   \n",
       "7332   now all we need is our astronaut. bart, where'...   \n",
       "\n",
       "                                                 context  \\\n",
       "4722    eek! -- i mean, ach! -- i mean, what are you ...   \n",
       "12154  hey, henrietta hippo, i've got to drain the in...   \n",
       "2695                                           nooooooo!   \n",
       "6511                 the crazy mayor of new springfield?   \n",
       "7332                      who wants some astro-lemonade?   \n",
       "\n",
       "                                               context/0  \\\n",
       "4722                                well, it used to be.   \n",
       "12154                        girling down! girling down!   \n",
       "2695   your mother and i have been thinking about giv...   \n",
       "6511                                       that's right.   \n",
       "7332                 dweeb, wonk, spaz... it's all good.   \n",
       "\n",
       "                                          context/1  \n",
       "4722   and now we wait.  so... is this your school?  \n",
       "12154   dad, where are we going? do we have a plan?  \n",
       "2695           well, i have to coordinate, don't i?  \n",
       "6511                  the mayor of new springfield?  \n",
       "7332   well, then, could you stop calling us nerds?  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size = 0.1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "def collate(examples):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.examples = []\n",
    "        for _, row in df.iterrows():\n",
    "            self.examples.append(construct_conv(row, tokenizer))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "WEIGHT_DECAY = 0.0\n",
    "LEARNING_RATE = 5e-5\n",
    "ADAM_EPSILON = 1e-8\n",
    "WARMUP_STEPS = 0\n",
    "EPOCH_NUM = 14\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "tensor([[30728,  7655,  3454,  4751, 13663,  7655, 18623, 21082, 16960, 17238,\n",
      "          1815, 15175,  5913,  4394, 13413, 10276,  9730,    16,  4810,  3618,\n",
      "          1797, 23144, 14991, 17181,  5126, 25492,  3519,    91,    82,  3618,\n",
      "          6452,    18, 50257,    76,  2672,  1188,    16, 14753, 18508,  4074,\n",
      "          6432,  2069,    18,  5245,  6351,    16,  7655,    74, 17181,    11,\n",
      "         24147, 19493,    71,  3379,    73, 20410,    16,  7655, 25660,  5913,\n",
      "          4074, 10749, 15446, 11037,  4074,  1815,  9851,  8989,    69,  5126,\n",
      "          3525, 14282,  2595,    83,    18, 50257,    72,  4680,    16,  3157,\n",
      "          1429,  7655, 17859,    79, 17181,  4074, 11158,  7702,  3771,    35,\n",
      "         50257,    87, 13663,    16,  3689, 13809,    93,    18, 50257]])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ConversationDataset(df, tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset) \n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=train_sampler,\n",
    "    batch_size=BATCH_SIZE, collate_fn=collate\n",
    ")\n",
    "\n",
    "for e in train_dataloader:\n",
    "    print('111')\n",
    "    print(e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, train_df, eval_df):\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    def collate(examples):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    train_dataset = ConversationDataset(train_df, tokenizer)\n",
    "    train_sampler = RandomSampler(train_dataset) \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler,\n",
    "        batch_size=BATCH_SIZE, collate_fn=collate\n",
    "    )\n",
    "    \n",
    "    eval_dataset = ConversationDataset(valid_df, tokenizer)\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler,\n",
    "        batch_size=BATCH_SIZE, collate_fn=collate\n",
    "    )\n",
    "    \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=ADAM_EPSILON)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=len(train_dataset) * EPOCH_NUM\n",
    "    )\n",
    "    \n",
    "    train_losses_epochs = []\n",
    "    eval_losses_epochs = []\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            inputs, labels = (batch, batch)\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "#             model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        eval_losses = []\n",
    "        for batch in eval_dataloader:\n",
    "            inputs, labels = (batch, batch)\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs, labels=labels)\n",
    "            eval_losses = outputs[0].item()\n",
    "            \n",
    "        train_losses_epochs.append(np.mean(train_losses))\n",
    "        eval_losses_epochs.append(np.mean(eval_losses))\n",
    "            \n",
    "        clear_output(True)          \n",
    "        sys.stdout.write('\\rEpoch {0}... (Train/Test) Loss: {1:.3f}/{2:.3f}, Perplexity: {1:.3f}/{2:.3f}'.format(\n",
    "                    epoch, train_losses_epochs[-1], eval_losses_epochs[-1], np.exp(train_losses_epochs[-1]), np.exp(eval_losses_epochs[-1])))\n",
    "        if epoch > 0:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.plot(train_losses_epochs[0:], label='Train')\n",
    "            plt.plot(eval_losses_epochs[0:], label='Valid')\n",
    "            plt.xlabel('Epochs', fontsize=16)\n",
    "            plt.ylabel('Loss', fontsize=16)\n",
    "            plt.legend(loc=0, fontsize=16)\n",
    "            plt.grid('on')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 1/10420 [00:56<162:31:10, 56.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-239205912d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-0c6277cf8530>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, tokenizer, train_df, eval_df)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, tokenizer, train_df, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt').to(DEVICE)\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Bot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('Homer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('Homer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained('Homer_model').to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained('Homer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    \n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Bot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
